{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import sys\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 29\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "char2id = {}\n",
    "id2char = {}\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "char_id = 0\n",
    "label_id = 0\n",
    "\n",
    "with open(\"data/train.txt\", \"rb\") as infile:\n",
    "    for row in infile:\n",
    "        row = row.strip().decode(\"utf-8\")\n",
    "        items = row.split()\n",
    "        if len(items) == 0:\n",
    "            sentences.append(x)\n",
    "            labels.append(y)\n",
    "            x = []\n",
    "            y = []\n",
    "        else:\n",
    "            i = char2id.setdefault(items[0], char_id)\n",
    "            id2char.setdefault(i, items[0])\n",
    "            if i == char_id:\n",
    "                char_id += 1\n",
    "            j = label2id.setdefault(items[1], label_id)\n",
    "            id2label.setdefault(j, items[1])\n",
    "            if j == label_id:\n",
    "                label_id += 1\n",
    "            x.append(i)\n",
    "            y.append(j)\n",
    "print \"data size: \" + str(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 206\n",
      "label size: 30\n",
      "train size: 23\n",
      "test size: 5\n"
     ]
    }
   ],
   "source": [
    "sentences = np.array(sentences)\n",
    "labels = np.array(labels)\n",
    "shuffle_data = True\n",
    "if shuffle_data:\n",
    "    sh = np.arange(len(sentences))\n",
    "    np.random.shuffle(sh)\n",
    "    sentences = sentences[sh]\n",
    "    labels = labels[sh]\n",
    "    \n",
    "train_size = int(len(sentences) * 0.8)\n",
    "test_size = int(len(sentences) * 0.2)\n",
    "\n",
    "X_train = sentences[:train_size]\n",
    "y_train = labels[:train_size]\n",
    "X_test = sentences[train_size:]\n",
    "y_test = labels[train_size:]\n",
    "\n",
    "vocabulary_size = len(id2char.keys())\n",
    "label_size = len(id2label.keys())\n",
    "\n",
    "print \"vocabulary size: \" + str(vocabulary_size)\n",
    "print \"label size: \" + str(label_size)\n",
    "print \"train size: \" + str(train_size)\n",
    "print \"test size: \" + str(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=20, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 5.327876\n",
      "Actual loss: 0.857475\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size)\n",
    "print \"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return \n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-07-19 10:27:10: Loss after num_examples_seen=0 epoch=0: 5.324849\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=23 epoch=1: 5.313896\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=46 epoch=2: 5.300916\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=69 epoch=3: 5.284040\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=92 epoch=4: 5.259822\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=115 epoch=5: 5.215450\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=138 epoch=6: 4.833301\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=161 epoch=7: 3.980198\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=184 epoch=8: 3.610934\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=207 epoch=9: 3.420193\n",
      "2016-07-19 10:27:10: Loss after num_examples_seen=230 epoch=10: 3.297428\n",
      "2016-07-19 10:27:11: Loss after num_examples_seen=253 epoch=11: 3.209645\n",
      "2016-07-19 10:27:11: Loss after num_examples_seen=276 epoch=12: 3.140909\n",
      "2016-07-19 10:27:11: Loss after num_examples_seen=299 epoch=13: 3.083155\n",
      "2016-07-19 10:27:11: Loss after num_examples_seen=322 epoch=14: 3.032269\n",
      "2016-07-19 10:27:11: Loss after num_examples_seen=345 epoch=15: 2.986094\n",
      "2016-07-19 10:27:11: Loss after num_examples_seen=368 epoch=16: 2.943530\n",
      "2016-07-19 10:27:11: Loss after num_examples_seen=391 epoch=17: 2.903893\n",
      "2016-07-19 10:27:11: Loss after num_examples_seen=414 epoch=18: 2.866619\n",
      "2016-07-19 10:27:11: Loss after num_examples_seen=437 epoch=19: 2.831121\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=460 epoch=20: 2.796773\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=483 epoch=21: 2.762975\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=506 epoch=22: 2.729234\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=529 epoch=23: 2.695165\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=552 epoch=24: 2.660550\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=575 epoch=25: 2.625636\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=598 epoch=26: 2.590769\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=621 epoch=27: 2.555893\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=644 epoch=28: 2.520946\n",
      "2016-07-19 10:27:12: Loss after num_examples_seen=667 epoch=29: 2.486018\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=690 epoch=30: 2.451274\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=713 epoch=31: 2.416880\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=736 epoch=32: 2.382942\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=759 epoch=33: 2.349484\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=782 epoch=34: 2.316464\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=805 epoch=35: 2.283821\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=828 epoch=36: 2.251503\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=851 epoch=37: 2.219492\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=874 epoch=38: 2.187845\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=897 epoch=39: 2.156831\n",
      "2016-07-19 10:27:13: Loss after num_examples_seen=920 epoch=40: 2.127295\n",
      "2016-07-19 10:27:14: Loss after num_examples_seen=943 epoch=41: 2.101014\n",
      "2016-07-19 10:27:14: Loss after num_examples_seen=966 epoch=42: 2.078827\n",
      "2016-07-19 10:27:14: Loss after num_examples_seen=989 epoch=43: 2.056241\n",
      "2016-07-19 10:27:14: Loss after num_examples_seen=1012 epoch=44: 2.029514\n",
      "2016-07-19 10:27:14: Loss after num_examples_seen=1035 epoch=45: 2.000407\n",
      "2016-07-19 10:27:14: Loss after num_examples_seen=1058 epoch=46: 1.969528\n",
      "2016-07-19 10:27:14: Loss after num_examples_seen=1081 epoch=47: 1.938102\n",
      "2016-07-19 10:27:14: Loss after num_examples_seen=1104 epoch=48: 1.906918\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1127 epoch=49: 1.877099\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1150 epoch=50: 1.848547\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1173 epoch=51: 1.818639\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1196 epoch=52: 1.782976\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1219 epoch=53: 1.734340\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1242 epoch=54: 1.680677\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1265 epoch=55: 1.656367\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1288 epoch=56: 1.633125\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1311 epoch=57: 1.609483\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1334 epoch=58: 1.578022\n",
      "2016-07-19 10:27:15: Loss after num_examples_seen=1357 epoch=59: 1.558028\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1380 epoch=60: 1.525005\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1403 epoch=61: 1.512485\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1426 epoch=62: 1.505811\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1449 epoch=63: 1.475819\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1472 epoch=64: 1.456105\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1495 epoch=65: 1.423910\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1518 epoch=66: 1.383504\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1541 epoch=67: 1.346972\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1564 epoch=68: 1.328614\n",
      "2016-07-19 10:27:16: Loss after num_examples_seen=1587 epoch=69: 1.305753\n",
      "2016-07-19 10:27:17: Loss after num_examples_seen=1610 epoch=70: 1.295250\n",
      "2016-07-19 10:27:17: Loss after num_examples_seen=1633 epoch=71: 1.314731\n",
      "Setting learning rate to 0.002500\n",
      "2016-07-19 10:27:17: Loss after num_examples_seen=1656 epoch=72: 1.219078\n",
      "2016-07-19 10:27:17: Loss after num_examples_seen=1679 epoch=73: 1.206049\n",
      "2016-07-19 10:27:17: Loss after num_examples_seen=1702 epoch=74: 1.190243\n",
      "2016-07-19 10:27:17: Loss after num_examples_seen=1725 epoch=75: 1.174996\n",
      "2016-07-19 10:27:17: Loss after num_examples_seen=1748 epoch=76: 1.160520\n",
      "2016-07-19 10:27:17: Loss after num_examples_seen=1771 epoch=77: 1.146569\n",
      "2016-07-19 10:27:18: Loss after num_examples_seen=1794 epoch=78: 1.132887\n",
      "2016-07-19 10:27:18: Loss after num_examples_seen=1817 epoch=79: 1.119306\n",
      "2016-07-19 10:27:18: Loss after num_examples_seen=1840 epoch=80: 1.105778\n",
      "2016-07-19 10:27:18: Loss after num_examples_seen=1863 epoch=81: 1.092358\n",
      "2016-07-19 10:27:18: Loss after num_examples_seen=1886 epoch=82: 1.079143\n",
      "2016-07-19 10:27:18: Loss after num_examples_seen=1909 epoch=83: 1.066234\n",
      "2016-07-19 10:27:18: Loss after num_examples_seen=1932 epoch=84: 1.053725\n",
      "2016-07-19 10:27:19: Loss after num_examples_seen=1955 epoch=85: 1.041689\n",
      "2016-07-19 10:27:19: Loss after num_examples_seen=1978 epoch=86: 1.030172\n",
      "2016-07-19 10:27:19: Loss after num_examples_seen=2001 epoch=87: 1.019175\n",
      "2016-07-19 10:27:19: Loss after num_examples_seen=2024 epoch=88: 1.008660\n",
      "2016-07-19 10:27:19: Loss after num_examples_seen=2047 epoch=89: 0.998563\n",
      "2016-07-19 10:27:19: Loss after num_examples_seen=2070 epoch=90: 0.988816\n",
      "2016-07-19 10:27:19: Loss after num_examples_seen=2093 epoch=91: 0.979357\n",
      "2016-07-19 10:27:20: Loss after num_examples_seen=2116 epoch=92: 0.970138\n",
      "2016-07-19 10:27:20: Loss after num_examples_seen=2139 epoch=93: 0.961121\n",
      "2016-07-19 10:27:20: Loss after num_examples_seen=2162 epoch=94: 0.952284\n",
      "2016-07-19 10:27:20: Loss after num_examples_seen=2185 epoch=95: 0.943610\n",
      "2016-07-19 10:27:20: Loss after num_examples_seen=2208 epoch=96: 0.935092\n",
      "2016-07-19 10:27:20: Loss after num_examples_seen=2231 epoch=97: 0.926728\n",
      "2016-07-19 10:27:20: Loss after num_examples_seen=2254 epoch=98: 0.918517\n",
      "2016-07-19 10:27:20: Loss after num_examples_seen=2277 epoch=99: 0.910462\n",
      "2016-07-19 10:27:20: Loss after num_examples_seen=2300 epoch=100: 0.902565\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2323 epoch=101: 0.894828\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2346 epoch=102: 0.887248\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2369 epoch=103: 0.879820\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2392 epoch=104: 0.872536\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2415 epoch=105: 0.865382\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2438 epoch=106: 0.858343\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2461 epoch=107: 0.851401\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2484 epoch=108: 0.844541\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2507 epoch=109: 0.837749\n",
      "2016-07-19 10:27:21: Loss after num_examples_seen=2530 epoch=110: 0.831015\n",
      "2016-07-19 10:27:22: Loss after num_examples_seen=2553 epoch=111: 0.824334\n",
      "2016-07-19 10:27:22: Loss after num_examples_seen=2576 epoch=112: 0.817704\n",
      "2016-07-19 10:27:22: Loss after num_examples_seen=2599 epoch=113: 0.811126\n",
      "2016-07-19 10:27:22: Loss after num_examples_seen=2622 epoch=114: 0.804602\n",
      "2016-07-19 10:27:22: Loss after num_examples_seen=2645 epoch=115: 0.798139\n",
      "2016-07-19 10:27:22: Loss after num_examples_seen=2668 epoch=116: 0.791743\n",
      "2016-07-19 10:27:22: Loss after num_examples_seen=2691 epoch=117: 0.785422\n",
      "2016-07-19 10:27:22: Loss after num_examples_seen=2714 epoch=118: 0.779184\n",
      "2016-07-19 10:27:23: Loss after num_examples_seen=2737 epoch=119: 0.773042\n",
      "2016-07-19 10:27:23: Loss after num_examples_seen=2760 epoch=120: 0.767008\n",
      "2016-07-19 10:27:23: Loss after num_examples_seen=2783 epoch=121: 0.761100\n",
      "2016-07-19 10:27:23: Loss after num_examples_seen=2806 epoch=122: 0.755335\n",
      "2016-07-19 10:27:23: Loss after num_examples_seen=2829 epoch=123: 0.749736\n",
      "2016-07-19 10:27:23: Loss after num_examples_seen=2852 epoch=124: 0.744326\n",
      "2016-07-19 10:27:23: Loss after num_examples_seen=2875 epoch=125: 0.739127\n",
      "2016-07-19 10:27:23: Loss after num_examples_seen=2898 epoch=126: 0.734158\n",
      "2016-07-19 10:27:24: Loss after num_examples_seen=2921 epoch=127: 0.729426\n",
      "2016-07-19 10:27:24: Loss after num_examples_seen=2944 epoch=128: 0.724917\n",
      "2016-07-19 10:27:24: Loss after num_examples_seen=2967 epoch=129: 0.720589\n",
      "2016-07-19 10:27:24: Loss after num_examples_seen=2990 epoch=130: 0.716370\n",
      "2016-07-19 10:27:24: Loss after num_examples_seen=3013 epoch=131: 0.712168\n",
      "2016-07-19 10:27:24: Loss after num_examples_seen=3036 epoch=132: 0.707892\n",
      "2016-07-19 10:27:24: Loss after num_examples_seen=3059 epoch=133: 0.703469\n",
      "2016-07-19 10:27:25: Loss after num_examples_seen=3082 epoch=134: 0.698849\n",
      "2016-07-19 10:27:25: Loss after num_examples_seen=3105 epoch=135: 0.694007\n",
      "2016-07-19 10:27:25: Loss after num_examples_seen=3128 epoch=136: 0.688933\n",
      "2016-07-19 10:27:25: Loss after num_examples_seen=3151 epoch=137: 0.683616\n",
      "2016-07-19 10:27:25: Loss after num_examples_seen=3174 epoch=138: 0.678043\n",
      "2016-07-19 10:27:25: Loss after num_examples_seen=3197 epoch=139: 0.672265\n",
      "2016-07-19 10:27:25: Loss after num_examples_seen=3220 epoch=140: 0.666414\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3243 epoch=141: 0.660595\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3266 epoch=142: 0.654810\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3289 epoch=143: 0.649080\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3312 epoch=144: 0.643474\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3335 epoch=145: 0.637968\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3358 epoch=146: 0.632486\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3381 epoch=147: 0.626986\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3404 epoch=148: 0.621450\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3427 epoch=149: 0.615924\n",
      "2016-07-19 10:27:26: Loss after num_examples_seen=3450 epoch=150: 0.610464\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3473 epoch=151: 0.605115\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3496 epoch=152: 0.599896\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3519 epoch=153: 0.594793\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3542 epoch=154: 0.589775\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3565 epoch=155: 0.584808\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3588 epoch=156: 0.579871\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3611 epoch=157: 0.574962\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3634 epoch=158: 0.570105\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3657 epoch=159: 0.565345\n",
      "2016-07-19 10:27:27: Loss after num_examples_seen=3680 epoch=160: 0.560713\n",
      "2016-07-19 10:27:28: Loss after num_examples_seen=3703 epoch=161: 0.556203\n",
      "2016-07-19 10:27:28: Loss after num_examples_seen=3726 epoch=162: 0.551779\n",
      "2016-07-19 10:27:28: Loss after num_examples_seen=3749 epoch=163: 0.547406\n",
      "2016-07-19 10:27:28: Loss after num_examples_seen=3772 epoch=164: 0.543076\n",
      "2016-07-19 10:27:28: Loss after num_examples_seen=3795 epoch=165: 0.538835\n",
      "2016-07-19 10:27:28: Loss after num_examples_seen=3818 epoch=166: 0.534768\n",
      "2016-07-19 10:27:28: Loss after num_examples_seen=3841 epoch=167: 0.530910\n",
      "2016-07-19 10:27:28: Loss after num_examples_seen=3864 epoch=168: 0.527201\n",
      "2016-07-19 10:27:28: Loss after num_examples_seen=3887 epoch=169: 0.523517\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=3910 epoch=170: 0.519673\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=3933 epoch=171: 0.515810\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=3956 epoch=172: 0.512415\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=3979 epoch=173: 0.509674\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=4002 epoch=174: 0.507162\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=4025 epoch=175: 0.503282\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=4048 epoch=176: 0.501691\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=4071 epoch=177: 0.497821\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=4094 epoch=178: 0.495039\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=4117 epoch=179: 0.494847\n",
      "2016-07-19 10:27:29: Loss after num_examples_seen=4140 epoch=180: 0.490633\n",
      "2016-07-19 10:27:30: Loss after num_examples_seen=4163 epoch=181: 0.482910\n",
      "2016-07-19 10:27:30: Loss after num_examples_seen=4186 epoch=182: 0.477356\n",
      "2016-07-19 10:27:30: Loss after num_examples_seen=4209 epoch=183: 0.474044\n",
      "2016-07-19 10:27:30: Loss after num_examples_seen=4232 epoch=184: 0.469122\n",
      "2016-07-19 10:27:30: Loss after num_examples_seen=4255 epoch=185: 0.477854\n",
      "Setting learning rate to 0.001250\n",
      "2016-07-19 10:27:30: Loss after num_examples_seen=4278 epoch=186: 0.470368\n",
      "2016-07-19 10:27:30: Loss after num_examples_seen=4301 epoch=187: 0.461306\n",
      "2016-07-19 10:27:30: Loss after num_examples_seen=4324 epoch=188: 0.458219\n",
      "2016-07-19 10:27:31: Loss after num_examples_seen=4347 epoch=189: 0.455802\n",
      "2016-07-19 10:27:31: Loss after num_examples_seen=4370 epoch=190: 0.453271\n",
      "2016-07-19 10:27:31: Loss after num_examples_seen=4393 epoch=191: 0.450441\n",
      "2016-07-19 10:27:31: Loss after num_examples_seen=4416 epoch=192: 0.447368\n",
      "2016-07-19 10:27:31: Loss after num_examples_seen=4439 epoch=193: 0.444197\n",
      "2016-07-19 10:27:31: Loss after num_examples_seen=4462 epoch=194: 0.441095\n",
      "2016-07-19 10:27:31: Loss after num_examples_seen=4485 epoch=195: 0.438178\n",
      "2016-07-19 10:27:31: Loss after num_examples_seen=4508 epoch=196: 0.435465\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4531 epoch=197: 0.432915\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4554 epoch=198: 0.430477\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4577 epoch=199: 0.428114\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4600 epoch=200: 0.425806\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4623 epoch=201: 0.423542\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4646 epoch=202: 0.421317\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4669 epoch=203: 0.419128\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4692 epoch=204: 0.416973\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4715 epoch=205: 0.414850\n",
      "2016-07-19 10:27:32: Loss after num_examples_seen=4738 epoch=206: 0.412757\n",
      "2016-07-19 10:27:33: Loss after num_examples_seen=4761 epoch=207: 0.410692\n",
      "2016-07-19 10:27:33: Loss after num_examples_seen=4784 epoch=208: 0.408652\n",
      "2016-07-19 10:27:33: Loss after num_examples_seen=4807 epoch=209: 0.406634\n",
      "2016-07-19 10:27:33: Loss after num_examples_seen=4830 epoch=210: 0.404637\n",
      "2016-07-19 10:27:33: Loss after num_examples_seen=4853 epoch=211: 0.402658\n",
      "2016-07-19 10:27:33: Loss after num_examples_seen=4876 epoch=212: 0.400695\n",
      "2016-07-19 10:27:33: Loss after num_examples_seen=4899 epoch=213: 0.398748\n",
      "2016-07-19 10:27:34: Loss after num_examples_seen=4922 epoch=214: 0.396813\n",
      "2016-07-19 10:27:34: Loss after num_examples_seen=4945 epoch=215: 0.394890\n",
      "2016-07-19 10:27:34: Loss after num_examples_seen=4968 epoch=216: 0.392978\n",
      "2016-07-19 10:27:34: Loss after num_examples_seen=4991 epoch=217: 0.391075\n",
      "2016-07-19 10:27:34: Loss after num_examples_seen=5014 epoch=218: 0.389182\n",
      "2016-07-19 10:27:34: Loss after num_examples_seen=5037 epoch=219: 0.387296\n",
      "2016-07-19 10:27:34: Loss after num_examples_seen=5060 epoch=220: 0.385418\n",
      "2016-07-19 10:27:35: Loss after num_examples_seen=5083 epoch=221: 0.383547\n",
      "2016-07-19 10:27:35: Loss after num_examples_seen=5106 epoch=222: 0.381683\n",
      "2016-07-19 10:27:35: Loss after num_examples_seen=5129 epoch=223: 0.379825\n",
      "2016-07-19 10:27:35: Loss after num_examples_seen=5152 epoch=224: 0.377974\n",
      "2016-07-19 10:27:35: Loss after num_examples_seen=5175 epoch=225: 0.376128\n",
      "2016-07-19 10:27:35: Loss after num_examples_seen=5198 epoch=226: 0.374289\n",
      "2016-07-19 10:27:35: Loss after num_examples_seen=5221 epoch=227: 0.372455\n",
      "2016-07-19 10:27:35: Loss after num_examples_seen=5244 epoch=228: 0.370628\n",
      "2016-07-19 10:27:35: Loss after num_examples_seen=5267 epoch=229: 0.368807\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5290 epoch=230: 0.366993\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5313 epoch=231: 0.365186\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5336 epoch=232: 0.363386\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5359 epoch=233: 0.361594\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5382 epoch=234: 0.359810\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5405 epoch=235: 0.358035\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5428 epoch=236: 0.356269\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5451 epoch=237: 0.354513\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5474 epoch=238: 0.352768\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5497 epoch=239: 0.351035\n",
      "2016-07-19 10:27:36: Loss after num_examples_seen=5520 epoch=240: 0.349313\n",
      "2016-07-19 10:27:37: Loss after num_examples_seen=5543 epoch=241: 0.347604\n",
      "2016-07-19 10:27:37: Loss after num_examples_seen=5566 epoch=242: 0.345908\n",
      "2016-07-19 10:27:37: Loss after num_examples_seen=5589 epoch=243: 0.344225\n",
      "2016-07-19 10:27:37: Loss after num_examples_seen=5612 epoch=244: 0.342556\n",
      "2016-07-19 10:27:37: Loss after num_examples_seen=5635 epoch=245: 0.340902\n",
      "2016-07-19 10:27:37: Loss after num_examples_seen=5658 epoch=246: 0.339262\n",
      "2016-07-19 10:27:37: Loss after num_examples_seen=5681 epoch=247: 0.337637\n",
      "2016-07-19 10:27:37: Loss after num_examples_seen=5704 epoch=248: 0.336026\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5727 epoch=249: 0.334431\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5750 epoch=250: 0.332850\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5773 epoch=251: 0.331284\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5796 epoch=252: 0.329732\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5819 epoch=253: 0.328194\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5842 epoch=254: 0.326669\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5865 epoch=255: 0.325159\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5888 epoch=256: 0.323661\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5911 epoch=257: 0.322175\n",
      "2016-07-19 10:27:38: Loss after num_examples_seen=5934 epoch=258: 0.320702\n",
      "2016-07-19 10:27:39: Loss after num_examples_seen=5957 epoch=259: 0.319241\n",
      "2016-07-19 10:27:39: Loss after num_examples_seen=5980 epoch=260: 0.317791\n",
      "2016-07-19 10:27:39: Loss after num_examples_seen=6003 epoch=261: 0.316352\n",
      "2016-07-19 10:27:39: Loss after num_examples_seen=6026 epoch=262: 0.314923\n",
      "2016-07-19 10:27:39: Loss after num_examples_seen=6049 epoch=263: 0.313505\n",
      "2016-07-19 10:27:39: Loss after num_examples_seen=6072 epoch=264: 0.312098\n",
      "2016-07-19 10:27:39: Loss after num_examples_seen=6095 epoch=265: 0.310699\n",
      "2016-07-19 10:27:39: Loss after num_examples_seen=6118 epoch=266: 0.309311\n",
      "2016-07-19 10:27:40: Loss after num_examples_seen=6141 epoch=267: 0.307931\n",
      "2016-07-19 10:27:40: Loss after num_examples_seen=6164 epoch=268: 0.306561\n",
      "2016-07-19 10:27:40: Loss after num_examples_seen=6187 epoch=269: 0.305199\n",
      "2016-07-19 10:27:40: Loss after num_examples_seen=6210 epoch=270: 0.303846\n",
      "2016-07-19 10:27:40: Loss after num_examples_seen=6233 epoch=271: 0.302502\n",
      "2016-07-19 10:27:40: Loss after num_examples_seen=6256 epoch=272: 0.301166\n",
      "2016-07-19 10:27:40: Loss after num_examples_seen=6279 epoch=273: 0.299838\n",
      "2016-07-19 10:27:41: Loss after num_examples_seen=6302 epoch=274: 0.298518\n",
      "2016-07-19 10:27:41: Loss after num_examples_seen=6325 epoch=275: 0.297207\n",
      "2016-07-19 10:27:41: Loss after num_examples_seen=6348 epoch=276: 0.295903\n",
      "2016-07-19 10:27:41: Loss after num_examples_seen=6371 epoch=277: 0.294607\n",
      "2016-07-19 10:27:41: Loss after num_examples_seen=6394 epoch=278: 0.293319\n",
      "2016-07-19 10:27:41: Loss after num_examples_seen=6417 epoch=279: 0.292039\n",
      "2016-07-19 10:27:41: Loss after num_examples_seen=6440 epoch=280: 0.290766\n",
      "2016-07-19 10:27:41: Loss after num_examples_seen=6463 epoch=281: 0.289502\n",
      "2016-07-19 10:27:41: Loss after num_examples_seen=6486 epoch=282: 0.288244\n",
      "2016-07-19 10:27:42: Loss after num_examples_seen=6509 epoch=283: 0.286995\n",
      "2016-07-19 10:27:42: Loss after num_examples_seen=6532 epoch=284: 0.285753\n",
      "2016-07-19 10:27:42: Loss after num_examples_seen=6555 epoch=285: 0.284518\n",
      "2016-07-19 10:27:42: Loss after num_examples_seen=6578 epoch=286: 0.283291\n",
      "2016-07-19 10:27:42: Loss after num_examples_seen=6601 epoch=287: 0.282072\n",
      "2016-07-19 10:27:42: Loss after num_examples_seen=6624 epoch=288: 0.280860\n",
      "2016-07-19 10:27:42: Loss after num_examples_seen=6647 epoch=289: 0.279656\n",
      "2016-07-19 10:27:42: Loss after num_examples_seen=6670 epoch=290: 0.278459\n",
      "2016-07-19 10:27:42: Loss after num_examples_seen=6693 epoch=291: 0.277270\n",
      "2016-07-19 10:27:43: Loss after num_examples_seen=6716 epoch=292: 0.276089\n",
      "2016-07-19 10:27:43: Loss after num_examples_seen=6739 epoch=293: 0.274915\n",
      "2016-07-19 10:27:43: Loss after num_examples_seen=6762 epoch=294: 0.273748\n",
      "2016-07-19 10:27:43: Loss after num_examples_seen=6785 epoch=295: 0.272589\n",
      "2016-07-19 10:27:43: Loss after num_examples_seen=6808 epoch=296: 0.271438\n",
      "2016-07-19 10:27:43: Loss after num_examples_seen=6831 epoch=297: 0.270294\n",
      "2016-07-19 10:27:43: Loss after num_examples_seen=6854 epoch=298: 0.269158\n",
      "2016-07-19 10:27:43: Loss after num_examples_seen=6877 epoch=299: 0.268030\n",
      "2016-07-19 10:27:44: Loss after num_examples_seen=6900 epoch=300: 0.266909\n",
      "2016-07-19 10:27:44: Loss after num_examples_seen=6923 epoch=301: 0.265795\n",
      "2016-07-19 10:27:44: Loss after num_examples_seen=6946 epoch=302: 0.264690\n",
      "2016-07-19 10:27:44: Loss after num_examples_seen=6969 epoch=303: 0.263592\n",
      "2016-07-19 10:27:44: Loss after num_examples_seen=6992 epoch=304: 0.262501\n",
      "2016-07-19 10:27:44: Loss after num_examples_seen=7015 epoch=305: 0.261419\n",
      "2016-07-19 10:27:44: Loss after num_examples_seen=7038 epoch=306: 0.260344\n",
      "2016-07-19 10:27:44: Loss after num_examples_seen=7061 epoch=307: 0.259276\n",
      "2016-07-19 10:27:45: Loss after num_examples_seen=7084 epoch=308: 0.258216\n",
      "2016-07-19 10:27:45: Loss after num_examples_seen=7107 epoch=309: 0.257164\n",
      "2016-07-19 10:27:45: Loss after num_examples_seen=7130 epoch=310: 0.256120\n",
      "2016-07-19 10:27:45: Loss after num_examples_seen=7153 epoch=311: 0.255083\n",
      "2016-07-19 10:27:45: Loss after num_examples_seen=7176 epoch=312: 0.254054\n",
      "2016-07-19 10:27:45: Loss after num_examples_seen=7199 epoch=313: 0.253032\n",
      "2016-07-19 10:27:45: Loss after num_examples_seen=7222 epoch=314: 0.252019\n",
      "2016-07-19 10:27:45: Loss after num_examples_seen=7245 epoch=315: 0.251012\n",
      "2016-07-19 10:27:46: Loss after num_examples_seen=7268 epoch=316: 0.250013\n",
      "2016-07-19 10:27:46: Loss after num_examples_seen=7291 epoch=317: 0.249022\n",
      "2016-07-19 10:27:46: Loss after num_examples_seen=7314 epoch=318: 0.248038\n",
      "2016-07-19 10:27:46: Loss after num_examples_seen=7337 epoch=319: 0.247062\n",
      "2016-07-19 10:27:46: Loss after num_examples_seen=7360 epoch=320: 0.246093\n",
      "2016-07-19 10:27:46: Loss after num_examples_seen=7383 epoch=321: 0.245131\n",
      "2016-07-19 10:27:46: Loss after num_examples_seen=7406 epoch=322: 0.244177\n",
      "2016-07-19 10:27:46: Loss after num_examples_seen=7429 epoch=323: 0.243230\n",
      "2016-07-19 10:27:47: Loss after num_examples_seen=7452 epoch=324: 0.242291\n",
      "2016-07-19 10:27:47: Loss after num_examples_seen=7475 epoch=325: 0.241358\n",
      "2016-07-19 10:27:47: Loss after num_examples_seen=7498 epoch=326: 0.240433\n",
      "2016-07-19 10:27:47: Loss after num_examples_seen=7521 epoch=327: 0.239514\n",
      "2016-07-19 10:27:47: Loss after num_examples_seen=7544 epoch=328: 0.238603\n",
      "2016-07-19 10:27:47: Loss after num_examples_seen=7567 epoch=329: 0.237699\n",
      "2016-07-19 10:27:47: Loss after num_examples_seen=7590 epoch=330: 0.236801\n",
      "2016-07-19 10:27:47: Loss after num_examples_seen=7613 epoch=331: 0.235911\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7636 epoch=332: 0.235027\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7659 epoch=333: 0.234150\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7682 epoch=334: 0.233279\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7705 epoch=335: 0.232415\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7728 epoch=336: 0.231558\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7751 epoch=337: 0.230707\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7774 epoch=338: 0.229862\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7797 epoch=339: 0.229023\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7820 epoch=340: 0.228191\n",
      "2016-07-19 10:27:48: Loss after num_examples_seen=7843 epoch=341: 0.227365\n",
      "2016-07-19 10:27:49: Loss after num_examples_seen=7866 epoch=342: 0.226545\n",
      "2016-07-19 10:27:49: Loss after num_examples_seen=7889 epoch=343: 0.225730\n",
      "2016-07-19 10:27:49: Loss after num_examples_seen=7912 epoch=344: 0.224922\n",
      "2016-07-19 10:27:49: Loss after num_examples_seen=7935 epoch=345: 0.224120\n",
      "2016-07-19 10:27:49: Loss after num_examples_seen=7958 epoch=346: 0.223323\n",
      "2016-07-19 10:27:49: Loss after num_examples_seen=7981 epoch=347: 0.222532\n",
      "2016-07-19 10:27:49: Loss after num_examples_seen=8004 epoch=348: 0.221747\n",
      "2016-07-19 10:27:49: Loss after num_examples_seen=8027 epoch=349: 0.220967\n",
      "2016-07-19 10:27:50: Loss after num_examples_seen=8050 epoch=350: 0.220193\n",
      "2016-07-19 10:27:50: Loss after num_examples_seen=8073 epoch=351: 0.219424\n",
      "2016-07-19 10:27:50: Loss after num_examples_seen=8096 epoch=352: 0.218660\n",
      "2016-07-19 10:27:50: Loss after num_examples_seen=8119 epoch=353: 0.217902\n",
      "2016-07-19 10:27:50: Loss after num_examples_seen=8142 epoch=354: 0.217149\n",
      "2016-07-19 10:27:50: Loss after num_examples_seen=8165 epoch=355: 0.216401\n",
      "2016-07-19 10:27:50: Loss after num_examples_seen=8188 epoch=356: 0.215659\n",
      "2016-07-19 10:27:50: Loss after num_examples_seen=8211 epoch=357: 0.214921\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8234 epoch=358: 0.214188\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8257 epoch=359: 0.213460\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8280 epoch=360: 0.212737\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8303 epoch=361: 0.212019\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8326 epoch=362: 0.211306\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8349 epoch=363: 0.210597\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8372 epoch=364: 0.209893\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8395 epoch=365: 0.209193\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8418 epoch=366: 0.208498\n",
      "2016-07-19 10:27:51: Loss after num_examples_seen=8441 epoch=367: 0.207808\n",
      "2016-07-19 10:27:52: Loss after num_examples_seen=8464 epoch=368: 0.207122\n",
      "2016-07-19 10:27:52: Loss after num_examples_seen=8487 epoch=369: 0.206440\n",
      "2016-07-19 10:27:52: Loss after num_examples_seen=8510 epoch=370: 0.205762\n",
      "2016-07-19 10:27:52: Loss after num_examples_seen=8533 epoch=371: 0.205089\n",
      "2016-07-19 10:27:52: Loss after num_examples_seen=8556 epoch=372: 0.204420\n",
      "2016-07-19 10:27:52: Loss after num_examples_seen=8579 epoch=373: 0.203755\n",
      "2016-07-19 10:27:52: Loss after num_examples_seen=8602 epoch=374: 0.203094\n",
      "2016-07-19 10:27:52: Loss after num_examples_seen=8625 epoch=375: 0.202438\n",
      "2016-07-19 10:27:52: Loss after num_examples_seen=8648 epoch=376: 0.201785\n",
      "2016-07-19 10:27:53: Loss after num_examples_seen=8671 epoch=377: 0.201136\n",
      "2016-07-19 10:27:53: Loss after num_examples_seen=8694 epoch=378: 0.200491\n",
      "2016-07-19 10:27:53: Loss after num_examples_seen=8717 epoch=379: 0.199851\n",
      "2016-07-19 10:27:53: Loss after num_examples_seen=8740 epoch=380: 0.199213\n",
      "2016-07-19 10:27:53: Loss after num_examples_seen=8763 epoch=381: 0.198580\n",
      "2016-07-19 10:27:53: Loss after num_examples_seen=8786 epoch=382: 0.197951\n",
      "2016-07-19 10:27:53: Loss after num_examples_seen=8809 epoch=383: 0.197325\n",
      "2016-07-19 10:27:53: Loss after num_examples_seen=8832 epoch=384: 0.196703\n",
      "2016-07-19 10:27:54: Loss after num_examples_seen=8855 epoch=385: 0.196084\n",
      "2016-07-19 10:27:54: Loss after num_examples_seen=8878 epoch=386: 0.195469\n",
      "2016-07-19 10:27:54: Loss after num_examples_seen=8901 epoch=387: 0.194858\n",
      "2016-07-19 10:27:54: Loss after num_examples_seen=8924 epoch=388: 0.194250\n",
      "2016-07-19 10:27:54: Loss after num_examples_seen=8947 epoch=389: 0.193646\n",
      "2016-07-19 10:27:54: Loss after num_examples_seen=8970 epoch=390: 0.193045\n",
      "2016-07-19 10:27:55: Loss after num_examples_seen=8993 epoch=391: 0.192447\n",
      "2016-07-19 10:27:55: Loss after num_examples_seen=9016 epoch=392: 0.191853\n",
      "2016-07-19 10:27:55: Loss after num_examples_seen=9039 epoch=393: 0.191262\n",
      "2016-07-19 10:27:55: Loss after num_examples_seen=9062 epoch=394: 0.190674\n",
      "2016-07-19 10:27:55: Loss after num_examples_seen=9085 epoch=395: 0.190089\n",
      "2016-07-19 10:27:55: Loss after num_examples_seen=9108 epoch=396: 0.189508\n",
      "2016-07-19 10:27:55: Loss after num_examples_seen=9131 epoch=397: 0.188930\n",
      "2016-07-19 10:27:55: Loss after num_examples_seen=9154 epoch=398: 0.188355\n",
      "2016-07-19 10:27:55: Loss after num_examples_seen=9177 epoch=399: 0.187783\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9200 epoch=400: 0.187214\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9223 epoch=401: 0.186648\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9246 epoch=402: 0.186085\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9269 epoch=403: 0.185525\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9292 epoch=404: 0.184968\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9315 epoch=405: 0.184413\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9338 epoch=406: 0.183862\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9361 epoch=407: 0.183313\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9384 epoch=408: 0.182767\n",
      "2016-07-19 10:27:56: Loss after num_examples_seen=9407 epoch=409: 0.182224\n",
      "2016-07-19 10:27:57: Loss after num_examples_seen=9430 epoch=410: 0.181683\n",
      "2016-07-19 10:27:57: Loss after num_examples_seen=9453 epoch=411: 0.181145\n",
      "2016-07-19 10:27:57: Loss after num_examples_seen=9476 epoch=412: 0.180609\n",
      "2016-07-19 10:27:57: Loss after num_examples_seen=9499 epoch=413: 0.180077\n",
      "2016-07-19 10:27:57: Loss after num_examples_seen=9522 epoch=414: 0.179546\n",
      "2016-07-19 10:27:57: Loss after num_examples_seen=9545 epoch=415: 0.179018\n",
      "2016-07-19 10:27:57: Loss after num_examples_seen=9568 epoch=416: 0.178493\n",
      "2016-07-19 10:27:57: Loss after num_examples_seen=9591 epoch=417: 0.177970\n",
      "2016-07-19 10:27:57: Loss after num_examples_seen=9614 epoch=418: 0.177450\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9637 epoch=419: 0.176931\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9660 epoch=420: 0.176416\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9683 epoch=421: 0.175902\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9706 epoch=422: 0.175391\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9729 epoch=423: 0.174882\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9752 epoch=424: 0.174375\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9775 epoch=425: 0.173870\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9798 epoch=426: 0.173368\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9821 epoch=427: 0.172868\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9844 epoch=428: 0.172369\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9867 epoch=429: 0.171873\n",
      "2016-07-19 10:27:58: Loss after num_examples_seen=9890 epoch=430: 0.171379\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=9913 epoch=431: 0.170887\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=9936 epoch=432: 0.170397\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=9959 epoch=433: 0.169909\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=9982 epoch=434: 0.169423\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=10005 epoch=435: 0.168939\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=10028 epoch=436: 0.168457\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=10051 epoch=437: 0.167977\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=10074 epoch=438: 0.167499\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=10097 epoch=439: 0.167023\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=10120 epoch=440: 0.166548\n",
      "2016-07-19 10:27:59: Loss after num_examples_seen=10143 epoch=441: 0.166076\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10166 epoch=442: 0.165605\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10189 epoch=443: 0.165136\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10212 epoch=444: 0.164669\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10235 epoch=445: 0.164204\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10258 epoch=446: 0.163741\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10281 epoch=447: 0.163279\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10304 epoch=448: 0.162820\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10327 epoch=449: 0.162362\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10350 epoch=450: 0.161906\n",
      "2016-07-19 10:28:00: Loss after num_examples_seen=10373 epoch=451: 0.161452\n",
      "2016-07-19 10:28:01: Loss after num_examples_seen=10396 epoch=452: 0.160999\n",
      "2016-07-19 10:28:01: Loss after num_examples_seen=10419 epoch=453: 0.160549\n",
      "2016-07-19 10:28:01: Loss after num_examples_seen=10442 epoch=454: 0.160100\n",
      "2016-07-19 10:28:01: Loss after num_examples_seen=10465 epoch=455: 0.159653\n",
      "2016-07-19 10:28:01: Loss after num_examples_seen=10488 epoch=456: 0.159208\n",
      "2016-07-19 10:28:01: Loss after num_examples_seen=10511 epoch=457: 0.158765\n",
      "2016-07-19 10:28:01: Loss after num_examples_seen=10534 epoch=458: 0.158323\n",
      "2016-07-19 10:28:02: Loss after num_examples_seen=10557 epoch=459: 0.157884\n",
      "2016-07-19 10:28:02: Loss after num_examples_seen=10580 epoch=460: 0.157446\n",
      "2016-07-19 10:28:02: Loss after num_examples_seen=10603 epoch=461: 0.157010\n",
      "2016-07-19 10:28:02: Loss after num_examples_seen=10626 epoch=462: 0.156576\n",
      "2016-07-19 10:28:02: Loss after num_examples_seen=10649 epoch=463: 0.156144\n",
      "2016-07-19 10:28:02: Loss after num_examples_seen=10672 epoch=464: 0.155714\n",
      "2016-07-19 10:28:03: Loss after num_examples_seen=10695 epoch=465: 0.155285\n",
      "2016-07-19 10:28:03: Loss after num_examples_seen=10718 epoch=466: 0.154858\n",
      "2016-07-19 10:28:03: Loss after num_examples_seen=10741 epoch=467: 0.154434\n",
      "2016-07-19 10:28:03: Loss after num_examples_seen=10764 epoch=468: 0.154011\n",
      "2016-07-19 10:28:03: Loss after num_examples_seen=10787 epoch=469: 0.153590\n",
      "2016-07-19 10:28:03: Loss after num_examples_seen=10810 epoch=470: 0.153171\n",
      "2016-07-19 10:28:03: Loss after num_examples_seen=10833 epoch=471: 0.152754\n",
      "2016-07-19 10:28:03: Loss after num_examples_seen=10856 epoch=472: 0.152339\n",
      "2016-07-19 10:28:03: Loss after num_examples_seen=10879 epoch=473: 0.151925\n",
      "2016-07-19 10:28:04: Loss after num_examples_seen=10902 epoch=474: 0.151514\n",
      "2016-07-19 10:28:04: Loss after num_examples_seen=10925 epoch=475: 0.151105\n",
      "2016-07-19 10:28:04: Loss after num_examples_seen=10948 epoch=476: 0.150697\n",
      "2016-07-19 10:28:04: Loss after num_examples_seen=10971 epoch=477: 0.150292\n",
      "2016-07-19 10:28:04: Loss after num_examples_seen=10994 epoch=478: 0.149888\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11017 epoch=479: 0.149486\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11040 epoch=480: 0.149087\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11063 epoch=481: 0.148689\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11086 epoch=482: 0.148293\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11109 epoch=483: 0.147899\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11132 epoch=484: 0.147507\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11155 epoch=485: 0.147117\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11178 epoch=486: 0.146729\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11201 epoch=487: 0.146343\n",
      "2016-07-19 10:28:05: Loss after num_examples_seen=11224 epoch=488: 0.145959\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11247 epoch=489: 0.145574\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11270 epoch=490: 0.145188\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11293 epoch=491: 0.144811\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11316 epoch=492: 0.144454\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11339 epoch=493: 0.144099\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11362 epoch=494: 0.143679\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11385 epoch=495: 0.143264\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11408 epoch=496: 0.142870\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11431 epoch=497: 0.142941\n",
      "Setting learning rate to 0.000625\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11454 epoch=498: 0.143380\n",
      "Setting learning rate to 0.000313\n",
      "2016-07-19 10:28:06: Loss after num_examples_seen=11477 epoch=499: 0.141549\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train, y_train, nepoch=500, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "右侧肾脏及右侧输尿管未见异常。\n",
      "[u'EOB', u'EOM', u'EOE', u'EAB', u'EAE', u'EOB', u'EOE', u'OOO', u'EOM', u'EAE', u'MNB', u'MNE', u'MSB', u'MSE', u'OOO']\n",
      "心脏可见增大，心包可见少量积液。\n",
      "[u'EOB', u'EOE', u'OOO', u'OOO', u'MSB', u'MSE', u'OOO', u'EAB', u'EAE', u'OOO', u'EAB', u'OOO', u'EOM', u'EOE', u'EPSE', u'OOO']\n",
      "部分椎体前后缘轻度骨质增生。\n",
      "[u'MSB', u'EAB', u'EAE', u'EOM', u'EOE', u'MSB', u'EAB', u'OOO', u'MSB', u'EPSB', u'MSB', u'MSE', u'EPSE', u'OOO']\n",
      "脊柱轻度侧弯。\n",
      "[u'OOO', u'OOO', u'EAB', u'OOO', u'OOO', u'EOE', u'OOO']\n",
      "下胸腔见金属缝合影。\n",
      "[u'EEB', u'EEM', u'EEM', u'OOO', u'OOO', u'EOB', u'EOM', u'EOE', u'MSE', u'OOO']\n",
      "右侧额叶小片状低密度影，边界清。\n",
      "[u'EOB', u'EOM', u'EOM', u'EOM', u'EOE', u'EAB', u'EAE', u'MSB', u'MSE', u'MWE', u'EEPE', u'OOO', u'EAB', u'EAE', u'MSB', u'EAE']\n",
      "accuracy: 0.327868852459\n"
     ]
    }
   ],
   "source": [
    "hit = 0\n",
    "num = 0\n",
    "for record in zip(X_test, y_test):\n",
    "    x = record[0]\n",
    "    y_true = record[1]\n",
    "    y_pred = model.predict(x)\n",
    "    print ''.join([id2char[val] for val in x])\n",
    "    print [id2label[val] for val in y_pred]\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == label2id[\"OOO\"]:\n",
    "            continue\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            hit += 1\n",
    "        num += 1\n",
    "print \"accuracy: \" + str(1.0 * hit / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
